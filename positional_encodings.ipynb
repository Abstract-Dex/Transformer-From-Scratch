{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "max_length = 10 #max number of words that can be passed to the model\n",
    "d_model = 6 #dimension of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(\\text{k}, i) = \\sin\\bigg( \\frac{ \\text{k} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{k}, i) = \\cos\\bigg( \\frac{ \\text{k} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd}\n",
    "$$\n",
    "Here:\n",
    "\n",
    "* k: Position of an object in the input sequence, \n",
    "\n",
    "* d_model: Dimension of the output embedding space\n",
    "\n",
    "* 10,000 is the user defined scalar, set to 10,000 by the authors of [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.0000,  21.5443, 464.1590])\n"
     ]
    }
   ],
   "source": [
    "denominator = torch.pow(10000, torch.arange(0, d_model, 2).float()/d_model)\n",
    "print(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, max_length).float().reshape(-1,1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8415,  0.0464,  0.0022],\n",
       "         [ 0.9093,  0.0927,  0.0043],\n",
       "         [ 0.1411,  0.1388,  0.0065],\n",
       "         [-0.7568,  0.1846,  0.0086],\n",
       "         [-0.9589,  0.2300,  0.0108],\n",
       "         [-0.2794,  0.2749,  0.0129],\n",
       "         [ 0.6570,  0.3192,  0.0151],\n",
       "         [ 0.9894,  0.3629,  0.0172],\n",
       "         [ 0.4121,  0.4057,  0.0194]]),\n",
       " tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5403,  0.9989,  1.0000],\n",
       "         [-0.4161,  0.9957,  1.0000],\n",
       "         [-0.9900,  0.9903,  1.0000],\n",
       "         [-0.6536,  0.9828,  1.0000],\n",
       "         [ 0.2837,  0.9732,  0.9999],\n",
       "         [ 0.9602,  0.9615,  0.9999],\n",
       "         [ 0.7539,  0.9477,  0.9999],\n",
       "         [-0.1455,  0.9318,  0.9999],\n",
       "         [-0.9111,  0.9140,  0.9998]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE = torch.sin(position/denominator)\n",
    "odd_PE = torch.cos(position/denominator)\n",
    "\n",
    "even_PE, odd_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack((even_PE, odd_PE), dim=2)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_PE = stacked.reshape(max_length, d_model)\n",
    "stacked_PE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 3.7961e-01, -9.2515e-01, -6.2536e-01,  ...,  9.9995e-01,\n",
       "          1.0055e-02,  9.9995e-01],\n",
       "        [-5.7338e-01, -8.1929e-01,  2.8505e-01,  ...,  9.9994e-01,\n",
       "          1.0159e-02,  9.9995e-01],\n",
       "        [-9.9921e-01,  3.9821e-02,  9.5015e-01,  ...,  9.9994e-01,\n",
       "          1.0262e-02,  9.9995e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_length):\n",
    "    super().__init__()\n",
    "    self.max_length = max_length  \n",
    "    self.d_model = d_model\n",
    "\n",
    "  def forward(self):\n",
    "    denominator = torch.pow(10000, torch.arange(0, self.d_model, 2).float()/self.d_model)\n",
    "    position = torch.arange(0, self.max_length).float().reshape(-1,1)\n",
    "    even_PE = torch.sin(position/denominator)\n",
    "    odd_PE = torch.cos(position/denominator)\n",
    "    stacked = torch.stack((even_PE, odd_PE), dim=2)\n",
    "    stacked_PE = stacked.reshape(self.max_length, self.d_model)\n",
    "    return  stacked_PE\n",
    "  \n",
    "\n",
    "PE = PositionalEncoding(512, 100)\n",
    "PE.forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
